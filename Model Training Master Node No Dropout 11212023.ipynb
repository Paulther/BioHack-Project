{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65d3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import obonet\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "from Bio import SeqIO\n",
    "import Bio.PDB\n",
    "import urllib.request\n",
    "import py3Dmol\n",
    "import pylab\n",
    "import pickle as pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.nn import GENConv\n",
    "from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.pool import SAGPooling\n",
    "from torch_geometric.nn.aggr import MeanAggregation\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from Bio import PDB\n",
    "from rdkit import Chem\n",
    "import blosum as bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727cd78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    pdbfiles: str = \"/home/paul/BioHack/pdbind-refined-set/\"\n",
    "    AA_mol2_files: str = \"/home/paul/BioHack/AA_mol2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce9c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('atom2emb.pkl', 'rb') as f:\n",
    "    atom2emb = pickle.load(f)\n",
    "    \n",
    "with open('AA_embeddings_11172023.pkl', 'rb') as f:\n",
    "    AA_embeddings = pickle.load(f)\n",
    "    \n",
    "with open('bond_type_dict.pkl', 'rb') as f:\n",
    "    bond_type_dict = pickle.load(f)\n",
    "\n",
    "def get_atom_symbol(atomic_number):\n",
    "    return Chem.PeriodicTable.GetElementSymbol(Chem.GetPeriodicTable(), atomic_number)\n",
    "\n",
    "def remove_hetatm(input_pdb_file, output_pdb_file):\n",
    "    # Open the input PDB file for reading and the output PDB file for writing\n",
    "    with open(input_pdb_file, 'r') as infile, open(output_pdb_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            # Check if the line starts with 'HETATM' (non-protein atoms)\n",
    "            if line.startswith('HETATM'):\n",
    "                continue  # Skip this line (HETATM record)\n",
    "            # Write all other lines to the output file\n",
    "            outfile.write(line)\n",
    "            \n",
    "def get_atom_types_from_sdf(sdf_file):\n",
    "    supplier = Chem.SDMolSupplier(sdf_file)\n",
    "    atom_types = set()\n",
    "\n",
    "    for mol in supplier:\n",
    "        if mol is not None:\n",
    "            atoms = mol.GetAtoms()\n",
    "            atom_types.update([atom.GetSymbol() for atom in atoms])\n",
    "\n",
    "    return sorted(list(atom_types))\n",
    "\n",
    "def get_atom_types_from_mol2_split(mol2_file):\n",
    "    atom_types = set()\n",
    "\n",
    "    with open(mol2_file, 'r') as mol2:\n",
    "        reading_atoms = False\n",
    "        for line in mol2:\n",
    "            if line.strip() == '@<TRIPOS>ATOM':\n",
    "                reading_atoms = True\n",
    "                continue\n",
    "            elif line.strip() == '@<TRIPOS>BOND':\n",
    "                break\n",
    "\n",
    "            if reading_atoms:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 5:\n",
    "                    atom_type = parts[5]\n",
    "                    atom_types.add(atom_type)\n",
    "    \n",
    "    atom_types_split = set()\n",
    "    for atom in atom_types:\n",
    "        atom_types_split.add(str(atom).split('.')[0])\n",
    "        \n",
    "\n",
    "    return sorted(list(atom_types_split))\n",
    "\n",
    "def get_atom_types_from_mol2(mol2_file):\n",
    "    atom_types = set()\n",
    "\n",
    "    with open(mol2_file, 'r') as mol2:\n",
    "        reading_atoms = False\n",
    "        for line in mol2:\n",
    "            if line.strip() == '@<TRIPOS>ATOM':\n",
    "                reading_atoms = True\n",
    "                continue\n",
    "            elif line.strip() == '@<TRIPOS>BOND':\n",
    "                break\n",
    "\n",
    "            if reading_atoms:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 5:\n",
    "                    atom_type = parts[5]\n",
    "                    atom_types.add(atom_type)\n",
    "\n",
    "    return sorted(list(atom_types))\n",
    "\n",
    "def get_atom_list_from_mol2_split(mol2_file):\n",
    "    atoms = []\n",
    "    with open(mol2_file, 'r') as mol2:\n",
    "        reading_atoms = False\n",
    "        for line in mol2:\n",
    "            if line.strip() == '@<TRIPOS>ATOM':\n",
    "                reading_atoms = True\n",
    "                continue\n",
    "            elif line.strip() == '@<TRIPOS>BOND':\n",
    "                break\n",
    "\n",
    "            if reading_atoms:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 5:\n",
    "                    atom_type = parts[5]\n",
    "                    atoms.append(atom_type)\n",
    "    \n",
    "    atom_list = []\n",
    "    for atom in atoms:\n",
    "        atom_list.append(str(atom).split('.')[0])\n",
    "        \n",
    "\n",
    "    return atom_list\n",
    "\n",
    "def get_atom_list_from_mol2(mol2_file):\n",
    "    atoms = []\n",
    "    with open(mol2_file, 'r') as mol2:\n",
    "        reading_atoms = False\n",
    "        for line in mol2:\n",
    "            if line.strip() == '@<TRIPOS>ATOM':\n",
    "                reading_atoms = True\n",
    "                continue\n",
    "            elif line.strip() == '@<TRIPOS>BOND':\n",
    "                break\n",
    "\n",
    "            if reading_atoms:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 5:\n",
    "                    atom_type = parts[5]\n",
    "                    atoms.append(atom_type)\n",
    "\n",
    "    return atoms\n",
    "\n",
    "def get_bond_types_from_mol2(mol2_file):\n",
    "    bond_types = set()\n",
    "\n",
    "    with open(mol2_file, 'r') as mol2:\n",
    "        reading_bonds = False\n",
    "        for line in mol2:\n",
    "            if line.strip() == '@<TRIPOS>BOND':\n",
    "                reading_bonds = True\n",
    "                continue\n",
    "            elif reading_bonds and line.strip().startswith('@<TRIPOS>'):\n",
    "                break\n",
    "\n",
    "            if reading_bonds:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 4:\n",
    "                    bond_type = parts[3]\n",
    "                    bond_types.add(bond_type)\n",
    "\n",
    "    return sorted(list(bond_types))\n",
    "\n",
    "def read_mol2_bonds(mol2_file):\n",
    "    bonds = []\n",
    "    bond_types = []\n",
    "\n",
    "    with open(mol2_file, 'r') as mol2:\n",
    "        reading_bonds = False\n",
    "        for line in mol2:\n",
    "            if line.strip() == '@<TRIPOS>BOND':\n",
    "                reading_bonds = True\n",
    "                continue\n",
    "            elif reading_bonds and line.strip().startswith('@<TRIPOS>'):\n",
    "                break\n",
    "\n",
    "            if reading_bonds:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 4:\n",
    "                    atom1_index = int(parts[1])\n",
    "                    atom2_index = int(parts[2])\n",
    "                    bond_type = parts[3]\n",
    "                    bonds.append((atom1_index, atom2_index))\n",
    "                    bond_types.append(bond_type)\n",
    "\n",
    "    return bonds, bond_types\n",
    "\n",
    "def calc_residue_dist(residue_one, residue_two) :\n",
    "    \"\"\"Returns the C-alpha distance between two residues\"\"\"\n",
    "    diff_vector  = residue_one[\"CA\"].coord - residue_two[\"CA\"].coord\n",
    "    return np.sqrt(np.sum(diff_vector * diff_vector))\n",
    "\n",
    "def calc_dist_matrix(chain_one, chain_two) :\n",
    "    \"\"\"Returns a matrix of C-alpha distances between two chains\"\"\"\n",
    "    answer = np.zeros((len(chain_one), len(chain_two)), float)\n",
    "    for row, residue_one in enumerate(chain_one) :\n",
    "        for col, residue_two in enumerate(chain_two) :\n",
    "            answer[row, col] = calc_residue_dist(residue_one, residue_two)\n",
    "    return answer\n",
    "\n",
    "def calc_contact_map(uniID,map_distance):\n",
    "    pdb_code = uniID\n",
    "    pdb_filename = uniID+\"_pocket_clean.pdb\"\n",
    "    structure = Bio.PDB.PDBParser(QUIET = True).get_structure(pdb_code, (CFG.pdbfiles +'/'+pdb_code+'/'+pdb_filename))\n",
    "    model = structure[0]\n",
    "    flag1 = 0\n",
    "    flag2 = 0\n",
    "    idx = 0\n",
    "    index = []\n",
    "    chain_info = []\n",
    "    \n",
    "    for chain1 in model:\n",
    "        for resi in chain1:\n",
    "            index.append(idx)\n",
    "            idx += 1\n",
    "            chain_info.append([chain1.id,resi.id])\n",
    "        for chain2 in model:\n",
    "            if flag1 == 0:\n",
    "                dist_matrix = calc_dist_matrix(model[chain1.id], model[chain2.id])\n",
    "            else:\n",
    "                new_matrix = calc_dist_matrix(model[chain1.id], model[chain2.id])\n",
    "                dist_matrix = np.hstack((dist_matrix,new_matrix))\n",
    "            flag1 += 1\n",
    "        flag1 = 0\n",
    "        if flag2 == 0:\n",
    "            top_matrix = dist_matrix\n",
    "        else:\n",
    "            top_matrix = np.vstack((top_matrix,dist_matrix))\n",
    "        flag2 += 1\n",
    "    \n",
    "    contact_map = top_matrix < map_distance\n",
    "    return contact_map, index, chain_info\n",
    "\n",
    "one_letter_to_three_letter_dict = {'G':'gly',\n",
    "                                   'A':'ala',\n",
    "                                   'V':'val',\n",
    "                                   'C':'cys',\n",
    "                                   'P':'pro',\n",
    "                                   'L':'leu',\n",
    "                                   'I':'ile',\n",
    "                                   'M':'met',\n",
    "                                   'W':'trp',\n",
    "                                   'F':'phe',\n",
    "                                   'K':'lys',\n",
    "                                   'R':'arg',\n",
    "                                   'H':'his',\n",
    "                                   'S':'ser',\n",
    "                                   'T':'thr',\n",
    "                                   'Y':'tyr',\n",
    "                                   'N':'asn',\n",
    "                                   'Q':'gln',\n",
    "                                   'D':'asp',\n",
    "                                   'E':'glu'\n",
    "    \n",
    "}\n",
    "\n",
    "def BLOSUM_encode_single(seq,AA_dict):\n",
    "    allowed = set(\"gavcplimwfkrhstynqdeuogavcplimwfkrhstynqde\")\n",
    "    if not set(seq).issubset(allowed):\n",
    "        invalid = set(seq) - allowed\n",
    "        raise ValueError(f\"Sequence has broken AA: {invalid}\")\n",
    "    vec = AA_dict[seq]\n",
    "    return vec\n",
    "\n",
    "matrix = bl.BLOSUM(62)\n",
    "allowed_AA = \"GAVCPLIMWFKRHSTYNQDE\"\n",
    "BLOSUM_dict_three_letter = {}\n",
    "for i in allowed_AA:\n",
    "    vec = []\n",
    "    for j in allowed_AA:\n",
    "        vec.append(matrix[i][j])\n",
    "    BLOSUM_dict_three_letter.update({one_letter_to_three_letter_dict[i]:torch.Tensor(vec)})\n",
    "\n",
    "def uniID2graph(uniID,map_distance):\n",
    "    atom_name = 'CA'\n",
    "    node_feature = []\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    coord = []\n",
    "    contact_map, index, chain_info = calc_contact_map(uniID,map_distance)\n",
    "    pdb_code = uniID\n",
    "    pdb_filename = uniID+\"_pocket_clean.pdb\"\n",
    "    structure = Bio.PDB.PDBParser(QUIET = True).get_structure(pdb_code, (CFG.pdbfiles +'/'+pdb_code+'/'+pdb_filename))\n",
    "    model = structure[0]\n",
    "    \n",
    "    for i in index:\n",
    "        node_feature.append(AA_embeddings[model[chain_info[i][0]][chain_info[i][1]].get_resname()])\n",
    "        coord.append(model[chain_info[i][0]][chain_info[i][1]]['CA'].coord)\n",
    "        for j in index:\n",
    "            if contact_map[i,j] == 1:\n",
    "                edge_index.append([i,j])\n",
    "                diff_vector = model[chain_info[i][0]][chain_info[i][1]]['CA'].coord - model[chain_info[j][0]][chain_info[j][1]]['CA'].coord\n",
    "                dist = (np.sqrt(np.sum(diff_vector * diff_vector))/map_distance)\n",
    "                bond_type = bond_type_dict['nc']\n",
    "                edge_attr.append(np.hstack((dist,bond_type)))\n",
    "                            \n",
    "    edge_index = np.array(edge_index)\n",
    "    edge_index = edge_index.transpose()\n",
    "    edge_index = torch.Tensor(edge_index)\n",
    "    edge_index = edge_index.to(torch.int64)\n",
    "    edge_attr = torch.Tensor(edge_attr)\n",
    "    node_feature = torch.stack(node_feature)\n",
    "    graph = Data(x = node_feature, edge_index = edge_index,edge_attr = edge_attr)\n",
    "    return graph, coord\n",
    "\n",
    "def read_mol2_bonds_and_atoms(mol2_file):\n",
    "    bonds = []\n",
    "    bond_types = []\n",
    "    atom_types = {}\n",
    "    atom_coordinates = {}\n",
    "\n",
    "    with open(mol2_file, 'r') as mol2:\n",
    "        reading_bonds = False\n",
    "        reading_atoms = False\n",
    "        for line in mol2:\n",
    "            if line.strip() == '@<TRIPOS>BOND':\n",
    "                reading_bonds = True\n",
    "                continue\n",
    "            elif line.strip() == '@<TRIPOS>ATOM':\n",
    "                reading_atoms = True\n",
    "                continue\n",
    "            elif line.strip().startswith('@<TRIPOS>SUBSTRUCTURE'):\n",
    "                break\n",
    "            elif reading_bonds and line.strip().startswith('@<TRIPOS>'):\n",
    "                reading_bonds = False\n",
    "            elif reading_atoms and line.strip().startswith('@<TRIPOS>'):\n",
    "                reading_atoms = False\n",
    "\n",
    "\n",
    "            if reading_bonds:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 4:\n",
    "                    atom1_index = int(parts[1])\n",
    "                    atom2_index = int(parts[2])\n",
    "                    bond_type = parts[3]\n",
    "                    bonds.append((atom1_index, atom2_index))\n",
    "                    bond_types.append(bond_type)\n",
    "\n",
    "            if reading_atoms:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 6:\n",
    "                    atom_index = int(parts[0])\n",
    "                    atom_type = parts[5]\n",
    "                    x, y, z = float(parts[2]), float(parts[3]), float(parts[4])\n",
    "                    atom_types[atom_index] = atom_type.split('.')[0]\n",
    "                    atom_coordinates[atom_index] = (x, y, z)\n",
    "\n",
    "    return bonds, bond_types, atom_types, atom_coordinates\n",
    "\n",
    "def molecule2graph(filename,map_distance):\n",
    "    node_feature = []\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    mol2_file = CFG.pdbfiles+filename+'/'+filename+'_ligand.mol2'\n",
    "    bonds, bond_types, atom_types, atom_coordinates = read_mol2_bonds_and_atoms(mol2_file)\n",
    "    for atom in atom_types:\n",
    "        #node_feature.append(torch.zeros(20))\n",
    "        node_feature.append(torch.Tensor(atom2emb[atom_types[atom]]))\n",
    "    for i in range(len(bonds)):\n",
    "        bond = bonds[i]\n",
    "        edge_index.append([bond[0] - 1,bond[1] - 1])\n",
    "        coord1 = np.array(atom_coordinates[bond[0]])\n",
    "        coord2 = np.array(atom_coordinates[bond[1]])\n",
    "        dist = [np.sqrt(np.sum((coord1 - coord2)*(coord1 - coord2)))/map_distance]\n",
    "        bond_type = bond_type_dict[bond_types[i]]\n",
    "        edge_attr.append(np.hstack((dist,bond_type)))\n",
    "    \n",
    "    edge_index = np.array(edge_index)\n",
    "    edge_index = edge_index.transpose()\n",
    "    edge_index = torch.Tensor(edge_index)\n",
    "    edge_index = edge_index.to(torch.int64)\n",
    "    edge_attr = torch.Tensor(edge_attr)\n",
    "    node_feature = torch.stack(node_feature)\n",
    "    graph = Data(x = node_feature, edge_index = edge_index,edge_attr = edge_attr)\n",
    "    \n",
    "    return graph, atom_coordinates\n",
    "\n",
    "def id2fullgraph(filename, map_distance):\n",
    "    prot_graph, prot_coord = uniID2graph(filename,map_distance)\n",
    "    prot_graph = prot_graph.to('cpu')\n",
    "    mol_graph, mol_coord = molecule2graph(filename,map_distance)\n",
    "    mol_graph = mol_graph.to('cpu')\n",
    "    mol_coord = [mol_coord[i] for i in mol_coord]\n",
    "    node_features = torch.cat((prot_graph.x,mol_graph.x),dim = 0)\n",
    "    update_edge_index = mol_graph.edge_index + prot_graph.x.size()[0]\n",
    "    edge_index = torch.cat((prot_graph.edge_index,update_edge_index), dim = 1)\n",
    "    edge_attr = torch.cat((prot_graph.edge_attr,mol_graph.edge_attr), dim = 0)\n",
    "    \n",
    "    new_edge_index = []\n",
    "    new_edge_attr = []\n",
    "    for i in range(len(mol_coord)):\n",
    "        for j in range(len(prot_coord)):\n",
    "            dist_vec = mol_coord[i] - prot_coord[j]\n",
    "            dist = np.sqrt(np.sum(dist_vec*dist_vec))/map_distance\n",
    "            if dist < 1.0:\n",
    "                new_edge_index.append([j,i + len(prot_coord)])\n",
    "                new_edge_attr.append((np.hstack(([dist],bond_type_dict['nc']))))\n",
    "                \n",
    "    new_edge_index = np.array(new_edge_index)\n",
    "    new_edge_index = new_edge_index.transpose()\n",
    "    new_edge_index = torch.Tensor(new_edge_index)\n",
    "    new_edge_index = new_edge_index.to(torch.int64)\n",
    "    new_edge_attr = torch.Tensor(new_edge_attr)\n",
    "    \n",
    "    edge_index = torch.cat((edge_index,new_edge_index), dim = 1)\n",
    "    edge_attr = torch.cat((edge_attr,new_edge_attr), dim = 0)\n",
    "    \n",
    "    graph = Data(x = node_features, edge_index = edge_index,edge_attr = edge_attr)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def molecule2graph_AA(filename,map_distance):\n",
    "    node_feature = []\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    mol2_file = filename\n",
    "    bonds, bond_types, atom_types, atom_coordinates = read_mol2_bonds_and_atoms(mol2_file)\n",
    "    for atom in atom_types:\n",
    "        #node_feature.append(torch.zeros(20))\n",
    "        node_feature.append(torch.Tensor(atom2emb[atom_types[atom]]))\n",
    "    for i in range(len(bonds)):\n",
    "        bond = bonds[i]\n",
    "        edge_index.append([bond[0] - 1,bond[1] - 1])\n",
    "        coord1 = np.array(atom_coordinates[bond[0]])\n",
    "        coord2 = np.array(atom_coordinates[bond[1]])\n",
    "        dist = [np.sqrt(np.sum((coord1 - coord2)*(coord1 - coord2)))/map_distance]\n",
    "        bond_type = bond_type_dict[bond_types[i]]\n",
    "        edge_attr.append(np.hstack((dist,bond_type)))\n",
    "    \n",
    "    #Master_node\n",
    "    node_feature.append(torch.zeros(len(atom2emb['N'])))\n",
    "    \n",
    "    for i in range(len(node_feature) - 1):\n",
    "        edge_index.append([i,int(len(node_feature)-1)])\n",
    "        bond_type = bond_type_dict['1']\n",
    "        edge_attr.append(np.hstack((1.0,bond_type)))\n",
    "    \n",
    "    edge_index = np.array(edge_index)\n",
    "    edge_index = edge_index.transpose()\n",
    "    edge_index = torch.Tensor(edge_index)\n",
    "    edge_index = edge_index.to(torch.int64)\n",
    "    edge_attr = torch.Tensor(edge_attr)\n",
    "    node_feature = torch.stack(node_feature)\n",
    "    graph = Data(x = node_feature, edge_index = edge_index,edge_attr = edge_attr)\n",
    "    \n",
    "    return graph, atom_coordinates\n",
    "\n",
    "upper2lower = {\n",
    "    \"ala\": \"ALA\",\n",
    "    \"arg\": \"ARG\",\n",
    "    \"asn\": \"ASN\",\n",
    "    \"asp\": \"ASP\",\n",
    "    \"cys\": \"CYS\",\n",
    "    \"gln\": \"GLN\",\n",
    "    \"glu\": \"GLU\",\n",
    "    \"gly\": \"GLY\",\n",
    "    \"his\": \"HIS\",\n",
    "    \"ile\": \"ILE\",\n",
    "    \"leu\": \"LEU\",\n",
    "    \"lys\": \"LYS\",\n",
    "    \"met\": \"MET\",\n",
    "    \"phe\": \"PHE\",\n",
    "    \"pro\": \"PRO\",\n",
    "    \"ser\": \"SER\",\n",
    "    \"thr\": \"THR\",\n",
    "    \"trp\": \"TRP\",\n",
    "    \"tyr\": \"TYR\",\n",
    "    \"val\": \"VAL\",\n",
    "}\n",
    "\n",
    "def createMask(graph,indicies,num_masked):\n",
    "    size = graph.x.size()[0]\n",
    "    protein_mask = [False]*size\n",
    "    true_mask = [True] * num_masked\n",
    "    indicies_mask = [False]*(len(indicies) - num_masked)\n",
    "    design_mask = np.hstack((true_mask,indicies_mask))\n",
    "    random.shuffle(design_mask)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(protein_mask)):\n",
    "        if i in indicies:\n",
    "            protein_mask[i] = design_mask[count]\n",
    "            count += 1\n",
    "            \n",
    "    for i, j in enumerate(protein_mask):\n",
    "        if j == 1.0:\n",
    "            protein_mask[i] = True\n",
    "    \n",
    "    return protein_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d237f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list = torch.load('graphs_w_designable_indicies_mn_11172023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e18b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "smallest = 12\n",
    "count = 0\n",
    "graph_list_clean = []\n",
    "for entry in graph_list:\n",
    "    if len(entry.designable_indicies) >= smallest:\n",
    "        graph_list_clean.append(entry)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26f1d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, graph in enumerate(graph_list_clean):\n",
    "    graph_list_clean[i].mask = createMask(graph,graph.designable_indicies,int(len(graph.designable_indicies)))\n",
    "    graph_list_clean[i].inv_mask = [not i for i in graph_list_clean[i].mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59411f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = random.choice(graph_list_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41b22c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, False, True, False, True, False, True, True, True, True, False, True, False, False, True, True, True, True, False, False, False, False, False, True, True, True, True, False, True, False, False, False, False, True, False, True, False, True, False, True, True, True, True, True, True, False, False, False, True, True, True, True, False, False, False, False, False, False, True, False, True, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[False, True, False, True, False, True, False, False, False, False, True, False, True, True, False, False, False, False, True, True, True, True, True, False, False, False, False, True, False, True, True, True, True, False, True, False, True, False, True, False, False, False, False, False, False, True, True, True, False, False, False, False, True, True, True, True, True, True, False, True, False, False, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print(graph.mask)\n",
    "print(graph.inv_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72f59828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.node_feature_size = 133\n",
    "        self.node_feature_hidden_size = 250\n",
    "        self.node_feature_size_out = 133\n",
    "        self.dropout = 0.1\n",
    "        self.Droput = nn.Dropout(p = self.dropout)\n",
    "        self.conv1 = GENConv(self.node_feature_size,self.node_feature_hidden_size,aggr = 'mean',edge_dim = 7,num_layer = 3,norm = 'layer',expansion = 4)\n",
    "        self.conv2 = GATv2Conv(self.node_feature_hidden_size,self.node_feature_hidden_size, edge_dim = 7, heads = 7,concat = False, dropout = self.dropout)\n",
    "        self.conv3 = GENConv(self.node_feature_hidden_size,self.node_feature_hidden_size,aggr = 'mean',edge_dim = 7,num_layer = 3, norm = 'layer',expansion = 4)\n",
    "        self.conv4 = GATv2Conv(self.node_feature_hidden_size,self.node_feature_size_out, edge_dim = 7, heads = 7,concat = False, dropout = self.dropout)\n",
    "        self.ReLu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self,graph):\n",
    "        x, edge_index, edge_attr,inv_mask = graph.x,graph.edge_index,graph.edge_attr,graph.inv_mask\n",
    "        x1 = self.conv1(x, edge_index,edge_attr)\n",
    "        #x1[inv_mask] = x[inv_mask]\n",
    "        x1 = self.ReLu(x1)\n",
    "        x1 = self.conv2(x1, edge_index,edge_attr)\n",
    "        #x1[inv_mask] = x[inv_mask]\n",
    "        x1 = self.ReLu(x1)\n",
    "        x1 = self.conv3(x1, edge_index,edge_attr)\n",
    "        #x1[inv_mask] = x[inv_mask]\n",
    "        x1 = self.ReLu(x1)\n",
    "        x1 = self.conv4(x1, edge_index,edge_attr)\n",
    "        x1 = self.tanh(x1)\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "755ca0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,graph in enumerate(graphs):\n",
    "#    mask1 =  createMask(graph.label,graph,1)\n",
    "#    mask2 = createMask(graph.label,graph,1)\n",
    "#    if mask1.all() == mask2.all():\n",
    "#        mask2 = createMask(graph.label,graph,1)\n",
    "#        if mask1.all() == mask2.all():\n",
    "#            mask2 = createMask(graph.label,graph,1)\n",
    "#    graphs[i].train_mask = mask1\n",
    "#    graphs[i].val_mask = mask2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "330e07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(graph_list_clean,'full_graphs_mn_rm_12.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e47913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def quick_split(prot_list, split_frac=0.8):\n",
    "#    '''\n",
    "#    Given a df of samples, randomly split indices between\n",
    "#    train and test at the desired fraction\n",
    "#    '''\n",
    "#\n",
    "#    # shuffle indices\n",
    "#    idxs = list(range(len(prot_list)))\n",
    "#    random.shuffle(idxs)\n",
    "#\n",
    "#    # split shuffled index list by split_frac\n",
    "#    train_idxs = idxs[:split]\n",
    "#    test_idxs = idxs[split:]\n",
    "#    \n",
    "#    # split dfs and return\n",
    "#    train_data = [prot_list[i] for i in train_idxs]\n",
    "#    test_data = [prot_list[i] for i in test_idxs]\n",
    "#        \n",
    "#    return train_data, test_data, train_idxs, test_idxs\n",
    "    \n",
    "    \n",
    "#full_train_data, test_data,train_idxs, test_idxs = quick_split(graph_list_clean)\n",
    "#train_data, val_data,train_idxs, test_idxs = quick_split(graph_list_clean)\n",
    "\n",
    "#print(\"Train:\", len(train_data))\n",
    "#print(\"Val:\", len(val_data))\n",
    "#print(\"Test:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1988e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idxs = [torch.Tensor(train_idxs),torch.Tensor(test_idxs)]\n",
    "#torch.save(idxs, 'initial_training_idx_11092023.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "072a0e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = torch.load('initial_training_idx_11092023.pt')\n",
    "train_idxs = idxs[0].detach().numpy()\n",
    "val_idxs = idxs[1].detach().numpy()\n",
    "train_data = [graph_list_clean[int(i)] for i in train_idxs]\n",
    "val_data = [graph_list_clean[int(i)] for i in val_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82d594a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "full_dl = DataLoader(graph_list,batch_size = 1, shuffle = True)\n",
    "train_dl = DataLoader(train_data,batch_size = 1, shuffle = True)\n",
    "val_dl = DataLoader(val_data,batch_size = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5130484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = random.choice(graph_list)\n",
    "mask = createMask(graph,graph.designable_indicies,int(len(graph.designable_indicies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "768203be",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = graph.x[mask]\n",
    "with torch.no_grad():\n",
    "    for i, j in enumerate(mask):\n",
    "        if j == True:\n",
    "            graph.x[i] = torch.zeros(133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f8084e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, True, False, False, False, False, True, True, False, False, True, False, False, False, False, True, True, True, False, True, True, False, False, False, False, True, True, True, True, False, False, False, False, True, False, True, False, False, False, True, True, False, True, False, False, False, False, True, False, False, True, True, True, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(mask)\n",
    "print(graph.x[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00dc1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1022110/3944119679.py:31: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  truth = inputs.x[inputs.mask]\n",
      "/tmp/ipykernel_1022110/3944119679.py:40: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  loss = criterion(outputs[inputs.mask], truth)\n",
      "/tmp/ipykernel_1022110/3944119679.py:58: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  truth = inputs.x[inputs.mask]\n",
      "/tmp/ipykernel_1022110/3944119679.py:65: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  loss = criterion(outputs[inputs.mask], truth)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000] Loss: 0.4327 Val Loss: 0.4286\n",
      "Epoch [2/10000] Loss: 0.4281 Val Loss: 0.4240\n",
      "Epoch [3/10000] Loss: 0.4221 Val Loss: 0.4183\n",
      "Epoch [4/10000] Loss: 0.4164 Val Loss: 0.4199\n",
      "Epoch [5/10000] Loss: 0.4092 Val Loss: 0.4039\n",
      "Epoch [6/10000] Loss: 0.4033 Val Loss: 0.4019\n",
      "Epoch [7/10000] Loss: 0.3982 Val Loss: 0.3966\n",
      "Epoch [8/10000] Loss: 0.3943 Val Loss: 0.3939\n",
      "Epoch [9/10000] Loss: 0.3918 Val Loss: 0.3939\n",
      "Epoch [10/10000] Loss: 0.3887 Val Loss: 0.3883\n",
      "Epoch [11/10000] Loss: 0.3859 Val Loss: 0.3846\n",
      "Epoch [12/10000] Loss: 0.3826 Val Loss: 0.3835\n",
      "Epoch [13/10000] Loss: 0.3805 Val Loss: 0.3793\n",
      "Epoch [14/10000] Loss: 0.3770 Val Loss: 0.3779\n",
      "Epoch [15/10000] Loss: 0.3746 Val Loss: 0.3771\n",
      "Epoch [16/10000] Loss: 0.3725 Val Loss: 0.3767\n",
      "Epoch [17/10000] Loss: 0.3706 Val Loss: 0.3728\n",
      "Epoch [18/10000] Loss: 0.3689 Val Loss: 0.3688\n",
      "Epoch [19/10000] Loss: 0.3661 Val Loss: 0.3664\n",
      "Epoch [20/10000] Loss: 0.3637 Val Loss: 0.3673\n",
      "Epoch [21/10000] Loss: 0.3616 Val Loss: 0.3641\n",
      "Epoch [22/10000] Loss: 0.3589 Val Loss: 0.3630\n",
      "Epoch [23/10000] Loss: 0.3584 Val Loss: 0.3605\n",
      "Epoch [24/10000] Loss: 0.3541 Val Loss: 0.3640\n",
      "Epoch [25/10000] Loss: 0.3530 Val Loss: 0.3579\n",
      "Epoch [26/10000] Loss: 0.3505 Val Loss: 0.3561\n",
      "Epoch [27/10000] Loss: 0.3494 Val Loss: 0.3540\n",
      "Epoch [28/10000] Loss: 0.3470 Val Loss: 0.3508\n",
      "Epoch [29/10000] Loss: 0.3445 Val Loss: 0.3494\n",
      "Epoch [30/10000] Loss: 0.3432 Val Loss: 0.3487\n",
      "Epoch [31/10000] Loss: 0.3414 Val Loss: 0.3457\n",
      "Epoch [32/10000] Loss: 0.3400 Val Loss: 0.3450\n",
      "Epoch [33/10000] Loss: 0.3371 Val Loss: 0.3415\n",
      "Epoch [34/10000] Loss: 0.3357 Val Loss: 0.3457\n",
      "Epoch [35/10000] Loss: 0.3342 Val Loss: 0.3394\n",
      "Epoch [36/10000] Loss: 0.3319 Val Loss: 0.3370\n",
      "Epoch [37/10000] Loss: 0.3307 Val Loss: 0.3353\n",
      "Epoch [38/10000] Loss: 0.3282 Val Loss: 0.3356\n",
      "Epoch [39/10000] Loss: 0.3269 Val Loss: 0.3378\n",
      "Epoch [40/10000] Loss: 0.3259 Val Loss: 0.3339\n",
      "Epoch [41/10000] Loss: 0.3232 Val Loss: 0.3275\n",
      "Epoch [42/10000] Loss: 0.3216 Val Loss: 0.3315\n",
      "Epoch [43/10000] Loss: 0.3203 Val Loss: 0.3286\n",
      "Epoch [44/10000] Loss: 0.3195 Val Loss: 0.3302\n",
      "Epoch [45/10000] Loss: 0.3170 Val Loss: 0.3260\n",
      "Epoch [46/10000] Loss: 0.3158 Val Loss: 0.3252\n",
      "Epoch [47/10000] Loss: 0.3143 Val Loss: 0.3221\n",
      "Epoch [48/10000] Loss: 0.3127 Val Loss: 0.3226\n",
      "Epoch [49/10000] Loss: 0.3109 Val Loss: 0.3228\n",
      "Epoch [50/10000] Loss: 0.3099 Val Loss: 0.3227\n",
      "Epoch [51/10000] Loss: 0.3085 Val Loss: 0.3193\n",
      "Epoch [52/10000] Loss: 0.3073 Val Loss: 0.3190\n",
      "Epoch [53/10000] Loss: 0.3055 Val Loss: 0.3190\n",
      "Epoch [54/10000] Loss: 0.3043 Val Loss: 0.3170\n",
      "Epoch [55/10000] Loss: 0.3039 Val Loss: 0.3165\n",
      "Epoch [56/10000] Loss: 0.3020 Val Loss: 0.3159\n",
      "Epoch [57/10000] Loss: 0.3009 Val Loss: 0.3122\n",
      "Epoch [58/10000] Loss: 0.2991 Val Loss: 0.3119\n",
      "Epoch [59/10000] Loss: 0.2978 Val Loss: 0.3114\n",
      "Epoch [60/10000] Loss: 0.2967 Val Loss: 0.3108\n",
      "Epoch [61/10000] Loss: 0.2953 Val Loss: 0.3094\n",
      "Epoch [62/10000] Loss: 0.2944 Val Loss: 0.3074\n",
      "Epoch [63/10000] Loss: 0.2931 Val Loss: 0.3069\n",
      "Epoch [64/10000] Loss: 0.2916 Val Loss: 0.3077\n",
      "Epoch [65/10000] Loss: 0.2898 Val Loss: 0.3067\n",
      "Epoch [66/10000] Loss: 0.2888 Val Loss: 0.3037\n",
      "Epoch [67/10000] Loss: 0.2887 Val Loss: 0.3054\n",
      "Epoch [68/10000] Loss: 0.2868 Val Loss: 0.3036\n",
      "Epoch [69/10000] Loss: 0.2849 Val Loss: 0.3017\n",
      "Epoch [70/10000] Loss: 0.2844 Val Loss: 0.3051\n",
      "Epoch [71/10000] Loss: 0.2825 Val Loss: 0.3013\n",
      "Epoch [72/10000] Loss: 0.2827 Val Loss: 0.3015\n",
      "Epoch [73/10000] Loss: 0.2812 Val Loss: 0.2969\n",
      "Epoch [74/10000] Loss: 0.2801 Val Loss: 0.2954\n",
      "Epoch [75/10000] Loss: 0.2785 Val Loss: 0.2944\n",
      "Epoch [76/10000] Loss: 0.2773 Val Loss: 0.2971\n",
      "Epoch [77/10000] Loss: 0.2765 Val Loss: 0.2954\n",
      "Epoch [78/10000] Loss: 0.2759 Val Loss: 0.2930\n",
      "Epoch [79/10000] Loss: 0.2736 Val Loss: 0.2940\n",
      "Epoch [80/10000] Loss: 0.2738 Val Loss: 0.2936\n",
      "Epoch [81/10000] Loss: 0.2719 Val Loss: 0.2935\n",
      "Epoch [82/10000] Loss: 0.2714 Val Loss: 0.2932\n",
      "Epoch [83/10000] Loss: 0.2702 Val Loss: 0.2906\n",
      "Epoch [84/10000] Loss: 0.2697 Val Loss: 0.2891\n",
      "Epoch [85/10000] Loss: 0.2672 Val Loss: 0.2869\n",
      "Epoch [86/10000] Loss: 0.2663 Val Loss: 0.2877\n",
      "Epoch [87/10000] Loss: 0.2655 Val Loss: 0.2897\n",
      "Epoch [88/10000] Loss: 0.2654 Val Loss: 0.2872\n",
      "Epoch [89/10000] Loss: 0.2635 Val Loss: 0.2888\n",
      "Epoch [90/10000] Loss: 0.2629 Val Loss: 0.2884\n",
      "Epoch [91/10000] Loss: 0.2623 Val Loss: 0.2852\n",
      "Epoch [92/10000] Loss: 0.2623 Val Loss: 0.2831\n",
      "Epoch [93/10000] Loss: 0.2601 Val Loss: 0.2837\n",
      "Epoch [94/10000] Loss: 0.2591 Val Loss: 0.2818\n",
      "Epoch [95/10000] Loss: 0.2585 Val Loss: 0.2814\n",
      "Epoch [96/10000] Loss: 0.2576 Val Loss: 0.2829\n",
      "Epoch [97/10000] Loss: 0.2562 Val Loss: 0.2786\n",
      "Epoch [98/10000] Loss: 0.2556 Val Loss: 0.2788\n",
      "Epoch [99/10000] Loss: 0.2554 Val Loss: 0.2798\n",
      "Epoch [100/10000] Loss: 0.2536 Val Loss: 0.2793\n",
      "Epoch [101/10000] Loss: 0.2528 Val Loss: 0.2794\n",
      "Epoch [102/10000] Loss: 0.2526 Val Loss: 0.2730\n",
      "Epoch [103/10000] Loss: 0.2501 Val Loss: 0.2772\n",
      "Epoch [104/10000] Loss: 0.2496 Val Loss: 0.2762\n",
      "Epoch [105/10000] Loss: 0.2491 Val Loss: 0.2759\n",
      "Epoch [106/10000] Loss: 0.2495 Val Loss: 0.2754\n",
      "Epoch [107/10000] Loss: 0.2476 Val Loss: 0.2736\n",
      "Epoch [108/10000] Loss: 0.2474 Val Loss: 0.2754\n",
      "Epoch [109/10000] Loss: 0.2461 Val Loss: 0.2727\n",
      "Epoch [110/10000] Loss: 0.2456 Val Loss: 0.2708\n",
      "Epoch [111/10000] Loss: 0.2447 Val Loss: 0.2732\n",
      "Epoch [112/10000] Loss: 0.2435 Val Loss: 0.2695\n",
      "Epoch [113/10000] Loss: 0.2428 Val Loss: 0.2685\n",
      "Epoch [114/10000] Loss: 0.2410 Val Loss: 0.2703\n",
      "Epoch [115/10000] Loss: 0.2418 Val Loss: 0.2706\n",
      "Epoch [116/10000] Loss: 0.2401 Val Loss: 0.2713\n",
      "Epoch [117/10000] Loss: 0.2397 Val Loss: 0.2677\n",
      "Epoch [118/10000] Loss: 0.2385 Val Loss: 0.2674\n",
      "Epoch [119/10000] Loss: 0.2375 Val Loss: 0.2669\n",
      "Epoch [120/10000] Loss: 0.2370 Val Loss: 0.2655\n",
      "Epoch [121/10000] Loss: 0.2363 Val Loss: 0.2699\n",
      "Epoch [122/10000] Loss: 0.2365 Val Loss: 0.2667\n",
      "Epoch [123/10000] Loss: 0.2347 Val Loss: 0.2647\n",
      "Epoch [124/10000] Loss: 0.2341 Val Loss: 0.2658\n",
      "Epoch [125/10000] Loss: 0.2329 Val Loss: 0.2679\n",
      "Epoch [126/10000] Loss: 0.2333 Val Loss: 0.2669\n",
      "Epoch [127/10000] Loss: 0.2317 Val Loss: 0.2650\n",
      "Epoch [128/10000] Loss: 0.2307 Val Loss: 0.2682\n",
      "Epoch [129/10000] Loss: 0.2305 Val Loss: 0.2623\n",
      "Epoch [130/10000] Loss: 0.2290 Val Loss: 0.2629\n",
      "Epoch [131/10000] Loss: 0.2291 Val Loss: 0.2604\n",
      "Epoch [132/10000] Loss: 0.2280 Val Loss: 0.2588\n",
      "Epoch [133/10000] Loss: 0.2268 Val Loss: 0.2616\n",
      "Epoch [134/10000] Loss: 0.2260 Val Loss: 0.2594\n",
      "Epoch [135/10000] Loss: 0.2266 Val Loss: 0.2600\n",
      "Epoch [136/10000] Loss: 0.2259 Val Loss: 0.2571\n",
      "Epoch [137/10000] Loss: 0.2238 Val Loss: 0.2616\n",
      "Epoch [138/10000] Loss: 0.2241 Val Loss: 0.2612\n",
      "Epoch [139/10000] Loss: 0.2241 Val Loss: 0.2576\n",
      "Epoch [140/10000] Loss: 0.2237 Val Loss: 0.2578\n",
      "Epoch [141/10000] Loss: 0.2221 Val Loss: 0.2591\n",
      "Epoch [142/10000] Loss: 0.2209 Val Loss: 0.2556\n",
      "Epoch [143/10000] Loss: 0.2213 Val Loss: 0.2600\n",
      "Epoch [144/10000] Loss: 0.2205 Val Loss: 0.2577\n",
      "Epoch [145/10000] Loss: 0.2199 Val Loss: 0.2558\n",
      "Epoch [146/10000] Loss: 0.2192 Val Loss: 0.2552\n",
      "Epoch [147/10000] Loss: 0.2180 Val Loss: 0.2536\n",
      "Epoch [148/10000] Loss: 0.2178 Val Loss: 0.2574\n",
      "Epoch [149/10000] Loss: 0.2180 Val Loss: 0.2537\n",
      "Epoch [150/10000] Loss: 0.2163 Val Loss: 0.2567\n",
      "Epoch [151/10000] Loss: 0.2172 Val Loss: 0.2561\n",
      "Epoch [152/10000] Loss: 0.2151 Val Loss: 0.2525\n",
      "Epoch [153/10000] Loss: 0.2142 Val Loss: 0.2518\n",
      "Epoch [154/10000] Loss: 0.2147 Val Loss: 0.2527\n",
      "Epoch [155/10000] Loss: 0.2138 Val Loss: 0.2509\n",
      "Epoch [156/10000] Loss: 0.2131 Val Loss: 0.2519\n",
      "Epoch [157/10000] Loss: 0.2129 Val Loss: 0.2504\n",
      "Epoch [158/10000] Loss: 0.2112 Val Loss: 0.2515\n",
      "Epoch [159/10000] Loss: 0.2119 Val Loss: 0.2508\n",
      "Epoch [160/10000] Loss: 0.2120 Val Loss: 0.2492\n",
      "Epoch [161/10000] Loss: 0.2097 Val Loss: 0.2508\n",
      "Epoch [162/10000] Loss: 0.2101 Val Loss: 0.2519\n",
      "Epoch [163/10000] Loss: 0.2100 Val Loss: 0.2514\n",
      "Epoch [164/10000] Loss: 0.2080 Val Loss: 0.2478\n",
      "Epoch [165/10000] Loss: 0.2089 Val Loss: 0.2494\n",
      "Epoch [166/10000] Loss: 0.2067 Val Loss: 0.2471\n",
      "Epoch [167/10000] Loss: 0.2059 Val Loss: 0.2450\n",
      "Epoch [168/10000] Loss: 0.2056 Val Loss: 0.2474\n",
      "Epoch [169/10000] Loss: 0.2049 Val Loss: 0.2483\n",
      "Epoch [170/10000] Loss: 0.2053 Val Loss: 0.2460\n",
      "Epoch [171/10000] Loss: 0.2040 Val Loss: 0.2474\n",
      "Epoch [172/10000] Loss: 0.2039 Val Loss: 0.2458\n",
      "Epoch [173/10000] Loss: 0.2045 Val Loss: 0.2468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [174/10000] Loss: 0.2028 Val Loss: 0.2448\n",
      "Epoch [175/10000] Loss: 0.2028 Val Loss: 0.2436\n",
      "Epoch [176/10000] Loss: 0.2025 Val Loss: 0.2445\n",
      "Epoch [177/10000] Loss: 0.2010 Val Loss: 0.2455\n",
      "Epoch [178/10000] Loss: 0.2015 Val Loss: 0.2432\n",
      "Epoch [179/10000] Loss: 0.2013 Val Loss: 0.2413\n",
      "Epoch [180/10000] Loss: 0.1993 Val Loss: 0.2428\n",
      "Epoch [181/10000] Loss: 0.1983 Val Loss: 0.2423\n",
      "Epoch [182/10000] Loss: 0.1982 Val Loss: 0.2441\n",
      "Epoch [183/10000] Loss: 0.1974 Val Loss: 0.2411\n",
      "Epoch [184/10000] Loss: 0.1974 Val Loss: 0.2436\n",
      "Epoch [185/10000] Loss: 0.1975 Val Loss: 0.2391\n",
      "Epoch [186/10000] Loss: 0.1966 Val Loss: 0.2447\n",
      "Epoch [187/10000] Loss: 0.1971 Val Loss: 0.2417\n",
      "Epoch [188/10000] Loss: 0.1950 Val Loss: 0.2416\n",
      "Epoch [189/10000] Loss: 0.1956 Val Loss: 0.2430\n",
      "Epoch [190/10000] Loss: 0.1947 Val Loss: 0.2385\n",
      "Epoch [191/10000] Loss: 0.1959 Val Loss: 0.2405\n",
      "Epoch [192/10000] Loss: 0.1943 Val Loss: 0.2436\n",
      "Epoch [193/10000] Loss: 0.1939 Val Loss: 0.2412\n",
      "Epoch [194/10000] Loss: 0.1930 Val Loss: 0.2374\n",
      "Epoch [195/10000] Loss: 0.1924 Val Loss: 0.2409\n",
      "Epoch [196/10000] Loss: 0.1918 Val Loss: 0.2382\n",
      "Epoch [197/10000] Loss: 0.1917 Val Loss: 0.2388\n",
      "Epoch [198/10000] Loss: 0.1913 Val Loss: 0.2366\n",
      "Epoch [199/10000] Loss: 0.1909 Val Loss: 0.2391\n",
      "Epoch [200/10000] Loss: 0.1915 Val Loss: 0.2361\n",
      "Epoch [201/10000] Loss: 0.1910 Val Loss: 0.2372\n",
      "Epoch [202/10000] Loss: 0.1902 Val Loss: 0.2379\n",
      "Epoch [203/10000] Loss: 0.1889 Val Loss: 0.2378\n",
      "Epoch [204/10000] Loss: 0.1879 Val Loss: 0.2367\n",
      "Epoch [205/10000] Loss: 0.1874 Val Loss: 0.2403\n",
      "Epoch [206/10000] Loss: 0.1876 Val Loss: 0.2360\n",
      "Epoch [207/10000] Loss: 0.1869 Val Loss: 0.2373\n",
      "Epoch [208/10000] Loss: 0.1864 Val Loss: 0.2359\n",
      "Epoch [209/10000] Loss: 0.1865 Val Loss: 0.2354\n",
      "Epoch [210/10000] Loss: 0.1864 Val Loss: 0.2345\n",
      "Epoch [211/10000] Loss: 0.1854 Val Loss: 0.2366\n",
      "Epoch [212/10000] Loss: 0.1857 Val Loss: 0.2338\n",
      "Epoch [213/10000] Loss: 0.1849 Val Loss: 0.2379\n",
      "Epoch [214/10000] Loss: 0.1847 Val Loss: 0.2361\n",
      "Epoch [215/10000] Loss: 0.1842 Val Loss: 0.2338\n",
      "Epoch [216/10000] Loss: 0.1839 Val Loss: 0.2355\n",
      "Epoch [217/10000] Loss: 0.1836 Val Loss: 0.2326\n",
      "Epoch [218/10000] Loss: 0.1832 Val Loss: 0.2330\n",
      "Epoch [219/10000] Loss: 0.1831 Val Loss: 0.2333\n",
      "Epoch [220/10000] Loss: 0.1816 Val Loss: 0.2310\n",
      "Epoch [221/10000] Loss: 0.1817 Val Loss: 0.2335\n",
      "Epoch [222/10000] Loss: 0.1819 Val Loss: 0.2347\n",
      "Epoch [223/10000] Loss: 0.1803 Val Loss: 0.2352\n",
      "Epoch [224/10000] Loss: 0.1822 Val Loss: 0.2325\n",
      "Epoch [225/10000] Loss: 0.1797 Val Loss: 0.2330\n",
      "Epoch [226/10000] Loss: 0.1796 Val Loss: 0.2331\n",
      "Epoch [227/10000] Loss: 0.1800 Val Loss: 0.2314\n",
      "Epoch [228/10000] Loss: 0.1794 Val Loss: 0.2312\n",
      "Epoch [229/10000] Loss: 0.1787 Val Loss: 0.2339\n",
      "Epoch [230/10000] Loss: 0.1789 Val Loss: 0.2297\n",
      "Epoch [231/10000] Loss: 0.1782 Val Loss: 0.2301\n",
      "Epoch [232/10000] Loss: 0.1781 Val Loss: 0.2330\n",
      "Epoch [233/10000] Loss: 0.1770 Val Loss: 0.2329\n",
      "Epoch [234/10000] Loss: 0.1759 Val Loss: 0.2290\n",
      "Epoch [235/10000] Loss: 0.1761 Val Loss: 0.2294\n",
      "Epoch [236/10000] Loss: 0.1765 Val Loss: 0.2323\n",
      "Epoch [237/10000] Loss: 0.1760 Val Loss: 0.2300\n",
      "Epoch [238/10000] Loss: 0.1746 Val Loss: 0.2307\n",
      "Epoch [239/10000] Loss: 0.1749 Val Loss: 0.2273\n",
      "Epoch [240/10000] Loss: 0.1745 Val Loss: 0.2288\n",
      "Epoch [241/10000] Loss: 0.1746 Val Loss: 0.2281\n",
      "Epoch [242/10000] Loss: 0.1743 Val Loss: 0.2308\n",
      "Epoch [243/10000] Loss: 0.1732 Val Loss: 0.2274\n",
      "Epoch [244/10000] Loss: 0.1718 Val Loss: 0.2252\n",
      "Epoch [245/10000] Loss: 0.1723 Val Loss: 0.2300\n",
      "Epoch [246/10000] Loss: 0.1722 Val Loss: 0.2279\n",
      "Epoch [247/10000] Loss: 0.1723 Val Loss: 0.2269\n",
      "Epoch [248/10000] Loss: 0.1715 Val Loss: 0.2274\n",
      "Epoch [249/10000] Loss: 0.1714 Val Loss: 0.2285\n",
      "Epoch [250/10000] Loss: 0.1712 Val Loss: 0.2254\n",
      "Epoch [251/10000] Loss: 0.1706 Val Loss: 0.2263\n",
      "Epoch [252/10000] Loss: 0.1705 Val Loss: 0.2252\n",
      "Epoch [253/10000] Loss: 0.1702 Val Loss: 0.2256\n",
      "Epoch [254/10000] Loss: 0.1697 Val Loss: 0.2276\n",
      "Epoch [255/10000] Loss: 0.1688 Val Loss: 0.2276\n",
      "Epoch [256/10000] Loss: 0.1695 Val Loss: 0.2272\n",
      "Epoch [257/10000] Loss: 0.1692 Val Loss: 0.2236\n",
      "Epoch [258/10000] Loss: 0.1684 Val Loss: 0.2257\n",
      "Epoch [259/10000] Loss: 0.1682 Val Loss: 0.2277\n",
      "Epoch [260/10000] Loss: 0.1678 Val Loss: 0.2234\n",
      "Epoch [261/10000] Loss: 0.1681 Val Loss: 0.2256\n",
      "Epoch [262/10000] Loss: 0.1668 Val Loss: 0.2229\n",
      "Epoch [263/10000] Loss: 0.1662 Val Loss: 0.2234\n",
      "Epoch [264/10000] Loss: 0.1665 Val Loss: 0.2234\n",
      "Epoch [265/10000] Loss: 0.1658 Val Loss: 0.2234\n",
      "Epoch [266/10000] Loss: 0.1659 Val Loss: 0.2250\n",
      "Epoch [267/10000] Loss: 0.1660 Val Loss: 0.2240\n",
      "Epoch [268/10000] Loss: 0.1651 Val Loss: 0.2237\n",
      "Epoch [269/10000] Loss: 0.1655 Val Loss: 0.2286\n",
      "Epoch [270/10000] Loss: 0.1644 Val Loss: 0.2233\n",
      "Epoch [271/10000] Loss: 0.1653 Val Loss: 0.2224\n",
      "Epoch [272/10000] Loss: 0.1639 Val Loss: 0.2240\n",
      "Epoch [273/10000] Loss: 0.1639 Val Loss: 0.2243\n",
      "Epoch [274/10000] Loss: 0.1633 Val Loss: 0.2242\n",
      "Epoch [275/10000] Loss: 0.1626 Val Loss: 0.2220\n",
      "Epoch [276/10000] Loss: 0.1637 Val Loss: 0.2233\n",
      "Epoch [277/10000] Loss: 0.1633 Val Loss: 0.2211\n",
      "Epoch [278/10000] Loss: 0.1630 Val Loss: 0.2212\n",
      "Epoch [279/10000] Loss: 0.1627 Val Loss: 0.2228\n",
      "Epoch [280/10000] Loss: 0.1621 Val Loss: 0.2214\n",
      "Epoch [281/10000] Loss: 0.1604 Val Loss: 0.2212\n",
      "Epoch [282/10000] Loss: 0.1612 Val Loss: 0.2235\n",
      "Epoch [283/10000] Loss: 0.1615 Val Loss: 0.2206\n",
      "Epoch [284/10000] Loss: 0.1604 Val Loss: 0.2210\n",
      "Epoch [285/10000] Loss: 0.1605 Val Loss: 0.2186\n",
      "Epoch [286/10000] Loss: 0.1589 Val Loss: 0.2229\n",
      "Epoch [287/10000] Loss: 0.1585 Val Loss: 0.2197\n",
      "Epoch [288/10000] Loss: 0.1596 Val Loss: 0.2208\n",
      "Epoch [289/10000] Loss: 0.1600 Val Loss: 0.2198\n",
      "Epoch [290/10000] Loss: 0.1584 Val Loss: 0.2201\n",
      "Epoch [291/10000] Loss: 0.1592 Val Loss: 0.2185\n",
      "Epoch [292/10000] Loss: 0.1588 Val Loss: 0.2197\n",
      "Epoch [293/10000] Loss: 0.1580 Val Loss: 0.2196\n",
      "Epoch [294/10000] Loss: 0.1572 Val Loss: 0.2187\n",
      "Epoch [295/10000] Loss: 0.1570 Val Loss: 0.2179\n",
      "Epoch [296/10000] Loss: 0.1586 Val Loss: 0.2193\n",
      "Epoch [297/10000] Loss: 0.1579 Val Loss: 0.2206\n",
      "Epoch [298/10000] Loss: 0.1572 Val Loss: 0.2195\n",
      "Epoch [299/10000] Loss: 0.1567 Val Loss: 0.2233\n",
      "Epoch [300/10000] Loss: 0.1565 Val Loss: 0.2211\n",
      "Epoch [301/10000] Loss: 0.1557 Val Loss: 0.2190\n",
      "Epoch [302/10000] Loss: 0.1552 Val Loss: 0.2180\n",
      "Epoch [303/10000] Loss: 0.1551 Val Loss: 0.2201\n",
      "Epoch [304/10000] Loss: 0.1543 Val Loss: 0.2188\n",
      "Epoch [305/10000] Loss: 0.1543 Val Loss: 0.2199\n",
      "Epoch [306/10000] Loss: 0.1530 Val Loss: 0.2178\n",
      "Epoch [307/10000] Loss: 0.1548 Val Loss: 0.2157\n",
      "Epoch [308/10000] Loss: 0.1540 Val Loss: 0.2219\n",
      "Epoch [309/10000] Loss: 0.1536 Val Loss: 0.2199\n",
      "Epoch [310/10000] Loss: 0.1541 Val Loss: 0.2189\n",
      "Epoch [311/10000] Loss: 0.1533 Val Loss: 0.2166\n",
      "Epoch [312/10000] Loss: 0.1530 Val Loss: 0.2173\n",
      "Epoch [313/10000] Loss: 0.1530 Val Loss: 0.2162\n",
      "Epoch [314/10000] Loss: 0.1529 Val Loss: 0.2154\n",
      "Epoch [315/10000] Loss: 0.1525 Val Loss: 0.2163\n",
      "Epoch [316/10000] Loss: 0.1511 Val Loss: 0.2181\n",
      "Epoch [317/10000] Loss: 0.1520 Val Loss: 0.2154\n",
      "Epoch [318/10000] Loss: 0.1508 Val Loss: 0.2161\n",
      "Epoch [319/10000] Loss: 0.1512 Val Loss: 0.2153\n",
      "Epoch [320/10000] Loss: 0.1520 Val Loss: 0.2157\n",
      "Epoch [321/10000] Loss: 0.1505 Val Loss: 0.2136\n",
      "Epoch [322/10000] Loss: 0.1496 Val Loss: 0.2166\n",
      "Epoch [323/10000] Loss: 0.1500 Val Loss: 0.2165\n",
      "Epoch [324/10000] Loss: 0.1499 Val Loss: 0.2171\n",
      "Epoch [325/10000] Loss: 0.1495 Val Loss: 0.2161\n",
      "Epoch [326/10000] Loss: 0.1495 Val Loss: 0.2150\n",
      "Epoch [327/10000] Loss: 0.1481 Val Loss: 0.2160\n",
      "Epoch [328/10000] Loss: 0.1499 Val Loss: 0.2180\n",
      "Epoch [329/10000] Loss: 0.1491 Val Loss: 0.2159\n",
      "Epoch [330/10000] Loss: 0.1488 Val Loss: 0.2179\n",
      "Epoch [331/10000] Loss: 0.1478 Val Loss: 0.2160\n",
      "Epoch [332/10000] Loss: 0.1486 Val Loss: 0.2165\n",
      "Epoch [333/10000] Loss: 0.1476 Val Loss: 0.2158\n",
      "Epoch [334/10000] Loss: 0.1481 Val Loss: 0.2160\n",
      "Epoch [335/10000] Loss: 0.1470 Val Loss: 0.2106\n",
      "Epoch [336/10000] Loss: 0.1470 Val Loss: 0.2148\n",
      "Epoch [337/10000] Loss: 0.1463 Val Loss: 0.2141\n",
      "Epoch [338/10000] Loss: 0.1470 Val Loss: 0.2119\n",
      "Epoch [339/10000] Loss: 0.1469 Val Loss: 0.2137\n",
      "Epoch [340/10000] Loss: 0.1476 Val Loss: 0.2117\n",
      "Epoch [341/10000] Loss: 0.1446 Val Loss: 0.2128\n",
      "Epoch [342/10000] Loss: 0.1464 Val Loss: 0.2133\n",
      "Epoch [343/10000] Loss: 0.1464 Val Loss: 0.2108\n",
      "Epoch [344/10000] Loss: 0.1457 Val Loss: 0.2179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [345/10000] Loss: 0.1462 Val Loss: 0.2168\n",
      "Epoch [346/10000] Loss: 0.1452 Val Loss: 0.2127\n",
      "Epoch [347/10000] Loss: 0.1438 Val Loss: 0.2152\n",
      "Epoch [348/10000] Loss: 0.1456 Val Loss: 0.2099\n",
      "Epoch [349/10000] Loss: 0.1443 Val Loss: 0.2093\n",
      "Epoch [350/10000] Loss: 0.1436 Val Loss: 0.2134\n",
      "Epoch [351/10000] Loss: 0.1451 Val Loss: 0.2140\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Net()\n",
    "#model.load_state_dict(torch.load('large_model_ckpt_layer_norm1.pt'))\n",
    "model.to(DEVICE) # put on GPU\n",
    "\n",
    "# Define a loss function (e.g., Mean Squared Error) and an optimizer (e.g., Adam)\n",
    "criterion = loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-07)\n",
    "#optimizer.load_state_dict(torch.load('large_model_ckpt_opt_layer_norm1.pt'))\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000  # Adjust the number of epochs as needed\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "        \n",
    "    for batch in train_dl:\n",
    "        model.train()\n",
    "        inputs = batch[0].to(DEVICE)\n",
    "        \n",
    "        inputs.mask = createMask(inputs,inputs.designable_indicies,12)\n",
    "        #mask = createMask(inputs,inputs.designable_indicies,int(len(inputs.designable_indicies)))\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        truth = inputs.x[inputs.mask]\n",
    "        with torch.no_grad():\n",
    "            for i, j in enumerate(inputs.mask):\n",
    "                if j == True:\n",
    "                    inputs.x[i] = torch.zeros(133)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs[inputs.mask], truth)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        inputs= inputs.to('cpu')\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    for batch in val_dl:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            inputs = batch[0].to(DEVICE)\n",
    "            \n",
    "            inputs.mask = createMask(inputs,inputs.designable_indicies,12)\n",
    "            #mask = createMask(inputs,inputs.designable_indicies,int(len(inputs.designable_indicies)))\n",
    "        \n",
    "            truth = inputs.x[inputs.mask]\n",
    "            for i, j in enumerate(inputs.mask):\n",
    "                if j == True:\n",
    "                    inputs.x[i] = torch.zeros(133)\n",
    "                    \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs[inputs.mask], truth)\n",
    "        \n",
    "            inputs= inputs.to('cpu')\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    avg_val_loss = val_loss / len(val_dl)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f} Val Loss: {avg_val_loss:.4f}')\n",
    "    losses.append([avg_loss,avg_val_loss])\n",
    "    \n",
    "    #if epoch > 100:\n",
    "    #    torch.save(autoencoder,('autoencoder_98var_10062023_'+str(epoch)))\n",
    "\n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e35b9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'flexible_number_of_binding_sites_11172023.pt')\n",
    "#torch.save(optimizer.state_dict(),'flexible_number_of_binding_sites_opt_11172023.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea84f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(torch.Tensor(losses),'flexible_number_of_binding_sites_loss_11172023.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
